{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e05635d-3043-43d0-9ec0-4c43d51097d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c0971-c06b-41de-ba0e-59f232f6fea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f8855d-1ddf-4e13-b4ba-f8961547ad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('title', 'text')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('title', 'text')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply function to a specific column (e.g., 'text_column')\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stopwords)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Save cleaned data to a new CSV file\u001b[39;00m\n\u001b[0;32m     27\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_file.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('title', 'text')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Ensure the value is a string\n",
    "        words = text.split()  # Tokenize by splitting\n",
    "        words = [word for word in words if word.lower() not in stop_words]  # Remove stop words\n",
    "        return \" \".join(words)  # Reconstruct the sentence\n",
    "    return text  # Return as is if not a string\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# Apply function to a specific column (e.g., 'text_column')\n",
    "df['title',] = df['title','text'].apply(remove_stopwords)\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)\n",
    "\n",
    "print(\"Stop words removed and file saved as 'cleaned_file.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd15f0d9-e1e2-44e1-bf7c-afe927acf478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'text', 'url', 'top_img', 'authors', 'source',\n",
      "       'publish_date', 'movies', 'images', 'canonical_link', 'meta_data'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# Display column names\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533c7058-ffc0-4c44-9591-d9cf4b90b60a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed from all columns and saved as 'cleaned_file.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Ensure it's a string\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "        return \" \".join(words)\n",
    "    return text  # Return non-string values as is\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# Apply stopword removal to all columns\n",
    "df = df.map(remove_stopwords)\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)\n",
    "\n",
    "print(\"Stop words removed from all columns and saved as 'cleaned_file.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd9243f1-a399-4882-ab3e-5abe75f45e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_11652\\3433090218.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_text)  # Now apply text processing\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Apply cleaning function to all text columns\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)  \u001b[38;5;66;03m# Convert all columns to string format\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapplymap(clean_text)  \u001b[38;5;66;03m# Now apply text processing\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# df = df.map(clean_text)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Save cleaned data to a new CSV file\u001b[39;00m\n\u001b[0;32m     37\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_file.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10522\u001b[0m, in \u001b[0;36mDataFrame.applymap\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m  10473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10474\u001b[0m \u001b[38;5;124;03mApply a function to a Dataframe elementwise.\u001b[39;00m\n\u001b[0;32m  10475\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10515\u001b[0m \u001b[38;5;124;03m1  5  5\u001b[39;00m\n\u001b[0;32m  10516\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10517\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10518\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.applymap has been deprecated. Use DataFrame.map instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m  10519\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m  10520\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  10521\u001b[0m )\n\u001b[1;32m> 10522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap(func, na_action\u001b[38;5;241m=\u001b[39mna_action, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10468\u001b[0m, in \u001b[0;36mDataFrame.map\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m  10465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[0;32m  10466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39m_map_values(func, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m> 10468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(infer)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10466\u001b[0m, in \u001b[0;36mDataFrame.map.<locals>.infer\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m  10465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[1;32m> 10466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39m_map_values(func, na_action\u001b[38;5;241m=\u001b[39mna_action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# Ensure it's a string\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(text)  \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Lemmatization & Stopword removal\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text: remove stopwords, apply lowercasing, and lemmatization\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure it's a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]  # Lemmatization & Stopword removal\n",
    "        return \" \".join(words)\n",
    "    return text  # Return non-string values as is\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# Apply cleaning function to all text columns\n",
    "df = df.astype(str)  # Convert all columns to string format\n",
    "df = df.applymap(clean_text)  # Now apply text processing\n",
    "\n",
    "# df = df.map(clean_text)\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)\n",
    "\n",
    "print(\"Stopwords removed, text lowercased, lemmatized, and saved as 'cleaned_file.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd795888-e60f-4bcf-9c16-24ae42c5ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed, text lowercased, lemmatized, and saved as 'cleaned_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text: stopword removal, lowercasing, and lemmatization\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure it's a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]  # Lemmatization & Stopword removal\n",
    "        return \" \".join(words)\n",
    "    return text  # Return non-string values as is\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# Apply cleaning only to object (text) columns using `.apply()`\n",
    "text_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[text_columns] = df[text_columns].map(clean_text)\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)\n",
    "\n",
    "print(\"Stopwords removed, text lowercased, lemmatized, and saved as 'cleaned_file.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6b6788d-4f79-457a-a328-be1be60ed810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7798fb23-f583-4368-bbee-40e939b3de8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stopwords removed, text lowercased, punctuation removed, lemmatized, and saved as 'cleaned_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ðŸ”¹ Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ðŸ”¹ Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ðŸ”¹ Function to clean text (Stopwords, Lowercasing, Lemmatization, Punctuation Removal)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "        return \" \".join(words)\n",
    "    return text  # Return original non-string values as is\n",
    "\n",
    "# ðŸ”¹ Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# ðŸ”¹ Apply cleaning only to object (text) columns\n",
    "text_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[text_columns] = df[text_columns].astype(str).map(clean_text)  # Ensure strings & apply function\n",
    "\n",
    "# ðŸ”¹ Save cleaned data to a new CSV file\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Stopwords removed, text lowercased, punctuation removed, lemmatized, and saved as 'cleaned_file.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76575597-5939-44d4-895b-774b4ad32e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text cleaned, TF-IDF applied, and saved as 'cleaned_tfidf_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ðŸ”¹ Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ðŸ”¹ Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ðŸ”¹ Function to clean text (Stopwords, Lowercasing, Lemmatization, Punctuation Removal)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "        return \" \".join(words)\n",
    "    return text  # Return original non-string values as is\n",
    "\n",
    "# ðŸ”¹ Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# ðŸ”¹ Apply cleaning only to object (text) columns\n",
    "text_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[text_columns] = df[text_columns].astype(str).map(clean_text)  # Ensure strings & apply function\n",
    "\n",
    "# ðŸ”¹ Combine all text columns for TF-IDF\n",
    "df[\"combined_text\"] = df[text_columns].apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "# ðŸ”¹ Apply TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"combined_text\"])\n",
    "\n",
    "# ðŸ”¹ Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# ðŸ”¹ Save cleaned text + TF-IDF features to new CSV\n",
    "cleaned_filename = \"cleaned_tfidf_data.csv\"\n",
    "final_df = pd.concat([df.drop(columns=[\"combined_text\"]), tfidf_df], axis=1)  # Merge TF-IDF with original data\n",
    "final_df.to_csv(cleaned_filename, index=False)\n",
    "\n",
    "print(f\"âœ… Text cleaned, TF-IDF applied, and saved as '{cleaned_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eae641bb-8f5f-4166-a632-07e9aebe1df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text cleaned, TF-IDF & Word2Vec applied, and saved as 'cleaned_tfidf_word2vec_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ðŸ”¹ Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ðŸ”¹ Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ðŸ”¹ Function to clean text (Stopwords, Lowercasing, Lemmatization, Punctuation Removal)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "        return \" \".join(words)\n",
    "    return text  # Return original non-string values as is\n",
    "\n",
    "# ðŸ”¹ Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# ðŸ”¹ Apply cleaning only to object (text) columns\n",
    "text_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[text_columns] = df[text_columns].astype(str).map(clean_text)  # Ensure strings & apply function\n",
    "\n",
    "# ðŸ”¹ Combine all text columns for TF-IDF and Word2Vec\n",
    "df[\"combined_text\"] = df[text_columns].apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "# âœ… **Apply TF-IDF Vectorization**\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"combined_text\"])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# âœ… **Apply Word2Vec Embeddings**\n",
    "sentences = df[\"combined_text\"].apply(lambda x: x.split()).tolist()  # Convert text to list of words\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)  # Train Word2Vec model\n",
    "\n",
    "# ðŸ”¹ Function to get Word2Vec embedding for each sentence\n",
    "def get_sentence_embedding(text):\n",
    "    words = text.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if word_vectors:  # If word vectors exist, compute the mean\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:  # If no word vectors found, return a zero vector\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# âœ… **Apply Word2Vec to Combined Text**\n",
    "df[\"word2vec_embedding\"] = df[\"combined_text\"].apply(get_sentence_embedding)\n",
    "\n",
    "# Convert Word2Vec embeddings into DataFrame\n",
    "word2vec_df = pd.DataFrame(df[\"word2vec_embedding\"].to_list(), columns=[f\"w2v_{i}\" for i in range(100)])\n",
    "\n",
    "# ðŸ”¹ Save cleaned text + TF-IDF + Word2Vec features to new CSV\n",
    "cleaned_filename = \"cleaned_tfidf_word2vec_data.csv\"\n",
    "final_df = pd.concat([df.drop(columns=[\"combined_text\", \"word2vec_embedding\"]), tfidf_df, word2vec_df], axis=1)  # Merge all features\n",
    "final_df.to_csv(cleaned_filename, index=False)\n",
    "\n",
    "print(f\"âœ… Text cleaned, TF-IDF & Word2Vec applied, and saved as '{cleaned_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "716ba271-e59a-43e1-9612-5383b3b1b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'cnn furious embarrassed trump called one major thing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11652\\139579727.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;31m# ðŸ”¹ Train & Evaluate Each Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m\\nðŸ”¹ Model: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \"\"\"\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mlabelbin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;34m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         raise ValueError(\n\u001b[0;32m   1298\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m requires y to be passed, but the target y is None\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m                 raise ValueError(\n\u001b[0;32m   1015\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'cnn furious embarrassed trump called one major thing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ðŸ”¹ Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ðŸ”¹ Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ðŸ”¹ Function to clean text (Stopwords, Lowercasing, Lemmatization, Punctuation Removal)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "        return \" \".join(words)\n",
    "    return text  # Return original non-string values as is\n",
    "\n",
    "# ðŸ”¹ Load CSV file\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "# ðŸ”¹ Encode Labels (Assuming 'label' column exists: Fake=1, Real=0)\n",
    "label_encoder = LabelEncoder()\n",
    "df['id'] = label_encoder.fit_transform(df['id'])  # Convert text labels to numbers\n",
    "\n",
    "# ðŸ”¹ Apply cleaning only to object (text) columns\n",
    "text_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[text_columns] = df[text_columns].astype(str).map(clean_text)  # Ensure strings & apply function\n",
    "\n",
    "# ðŸ”¹ Combine all text columns for TF-IDF and Word2Vec\n",
    "df[\"combined_text\"] = df[text_columns].apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "# âœ… **Apply TF-IDF Vectorization**\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"combined_text\"])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# âœ… **Apply Word2Vec Embeddings**\n",
    "sentences = df[\"combined_text\"].apply(lambda x: x.split()).tolist()  # Convert text to list of words\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)  # Train Word2Vec model\n",
    "\n",
    "# ðŸ”¹ Function to get Word2Vec embedding for each sentence\n",
    "def get_sentence_embedding(text):\n",
    "    words = text.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if word_vectors:  # If word vectors exist, compute the mean\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:  # If no word vectors found, return a zero vector\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# âœ… **Apply Word2Vec to Combined Text**\n",
    "df[\"word2vec_embedding\"] = df[\"combined_text\"].apply(get_sentence_embedding)\n",
    "\n",
    "# Convert Word2Vec embeddings into DataFrame\n",
    "word2vec_df = pd.DataFrame(df[\"word2vec_embedding\"].to_list(), columns=[f\"w2v_{i}\" for i in range(100)])\n",
    "\n",
    "# ðŸ”¹ Merge All Features\n",
    "final_df = pd.concat([df.drop(columns=[\"combined_text\", \"word2vec_embedding\"]), tfidf_df, word2vec_df], axis=1)\n",
    "\n",
    "# ðŸ”¹ Splitting Data into Training & Testing Sets\n",
    "X = final_df.drop(columns=[\"id\"])  # Features\n",
    "y = final_df[\"id\"]  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… **Model Selection & Training**\n",
    "models = {\n",
    "    \"NaÃ¯ve Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# ðŸ”¹ Train & Evaluate Each Model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Model: {name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ðŸ”¹ Save cleaned text + TF-IDF + Word2Vec features to new CSV\n",
    "cleaned_filename = \"cleaned_tfidf_word2vec_data_with_models.csv\"\n",
    "final_df.to_csv(cleaned_filename, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Text cleaned, TF-IDF & Word2Vec applied, models trained, and results saved as '{cleaned_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ade320f-9d66-4098-a9ab-1463418acdaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Logistic Regression Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                                     CNNâ€™s Post-Debate Rigged Poll Bustedâ€¦Here Are The Real Results       0.00      0.00      0.00         0\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                                                    Website is Down For Maintenance       1.00      1.00      1.00         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.15      0.15      0.15        19\n",
      "                                                                                                       weighted avg       0.16      0.16      0.16        19\n",
      "\n",
      "\n",
      "ðŸ”¹ NaÃ¯ve Bayes Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                                     CNNâ€™s Post-Debate Rigged Poll Bustedâ€¦Here Are The Real Results       0.00      0.00      0.00         0\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                                                    Website is Down For Maintenance       1.00      1.00      1.00         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.15      0.15      0.15        19\n",
      "                                                                                                       weighted avg       0.16      0.16      0.16        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Random Forest Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                           BOOM! Charlotte Officials FINALLY Silence Rioters With One E P I C Announcement! [VIDEO]       0.00      0.00      0.00         0\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       0.33      1.00      0.50         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                               Congress MUST Act! Five Days Until Obama's Internet Giveaway [Video]       0.00      0.00      0.00         0\n",
      "       DISGUSTING! Because Of Hillary & Obama, NY Terrorist Will Get Better Treatment Than US Vets! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "             NYC Terrorist Ahmad Rahami Sued Police Department for â€˜Religious Persecutionâ€™ in 2011! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "          No WAY! Liberal Judge Just Made A Law For Black Males To Do This DISGUSTING Thing To Cops â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                                          Note In Arabic Found With NYC Bombâ€¦ Call Says There Will Be More Bombings       0.00      0.00      0.00         0\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                                                                                  Obama Pushes One World Government       0.00      0.00      0.00         0\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                      The Pope Flat Out Called Fox News Type Journalism â€˜Terrorismâ€™       0.00      0.00      0.00         0\n",
      "             Trump Just Made A Campaign Promise So Ridiculous It Makes â€˜Read My Lipsâ€™ Look Good â€¹ Opposition Report       0.00      0.00      0.00         0\n",
      "                                                                                    Website is Down For Maintenance       0.20      1.00      0.33         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                                                â€˜We Donâ€™t Adapt To Your Culture, You Adapt To Oursâ€™       0.00      0.00      0.00         0\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.05      0.10      0.06        19\n",
      "                                                                                                       weighted avg       0.08      0.16      0.10        19\n",
      "\n",
      "\n",
      "ðŸ”¹ SVM Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                                                             Australia Voted To Ban Muslims And Liberals Are Pissed       0.00      0.00      0.00         0\n",
      "                                                          BOOM! Merkel Admits Flooding Germany With Muslim Refugees       0.00      0.00      0.00         0\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                                          BREAKING: WikiLeaks Releases Proof Hillary Lied and The FBI Covered It Up       0.00      0.00      0.00         0\n",
      "                                                                Bill And Hillary Clinton Have A MAJOR AIDS Scandal!       0.00      0.00      0.00         0\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                        CNN FURIOUS and Embarrassed After Trump Called Them Out For One Major Thing       0.00      0.00      0.00         0\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       0.50      1.00      0.67         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                 Hillary Denies She and Obama Founded ISISâ€¦Then This Video Shows Up       0.00      0.00      0.00         0\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "             NYC Terrorist Ahmad Rahami Sued Police Department for â€˜Religious Persecutionâ€™ in 2011! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "          No WAY! Liberal Judge Just Made A Law For Black Males To Do This DISGUSTING Thing To Cops â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                                                                                  Obama Pushes One World Government       0.00      0.00      0.00         0\n",
      "                                       Obamaâ€™s Speech In New York Shocks Americans When He Says THIS About Attacks!       0.00      0.00      0.00         0\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                             SURPRISE! 70% Of Charlotte RIOTERS Are Not From There â€“ Guess What That Meansâ€¦ [VIDEO]       0.00      0.00      0.00         0\n",
      "                                  Tebow DISMANTLES national anthem protests in one sentence...BOOM! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Top Army Chief of Staff General Has Had It With Obama Incompetence â‹† US Herald       0.00      0.00      0.00         0\n",
      "             Trump Just Made A Campaign Promise So Ridiculous It Makes â€˜Read My Lipsâ€™ Look Good â€¹ Opposition Report       0.00      0.00      0.00         0\n",
      "                                                   Trumpâ€™s Latest Campaign Promise May Be His Most Horrible One Yet       0.00      0.00      0.00         0\n",
      "                                                                                    Website is Down For Maintenance       1.00      1.00      1.00         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.07      0.09      0.08        19\n",
      "                                                                                                       weighted avg       0.13      0.16      0.14        19\n",
      "\n",
      "âœ… Model training and evaluation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ðŸ”¹ Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ðŸ”¹ Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ðŸ”¹ Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "        return \" \".join(words)\n",
    "    return text  # Return original non-string values as is\n",
    "\n",
    "# ðŸ”¹ Load dataset\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "df['text'] = df.select_dtypes(include=[\"object\"]).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# ðŸ”¹ Define target variable (assuming there is a 'label' column with 1 for Fake, 0 for Real)\n",
    "if 'title' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'label' column for classification.\")\n",
    "X = df['text']\n",
    "y = df['title']\n",
    "\n",
    "# âœ… **Apply TF-IDF Vectorization**\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# ðŸ”¹ Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… **Train and Evaluate Models**\n",
    "def train_and_evaluate(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nðŸ”¹ {model_name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"NaÃ¯ve Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    train_and_evaluate(model, name)\n",
    "\n",
    "print(\"âœ… Model training and evaluation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4b15682-53a5-46e7-bc28-0f2e689c8106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Logistic Regression Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                                     CNNâ€™s Post-Debate Rigged Poll Bustedâ€¦Here Are The Real Results       0.00      0.00      0.00         0\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                                                    Website is Down For Maintenance       1.00      1.00      1.00         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.15      0.15      0.15        19\n",
      "                                                                                                       weighted avg       0.16      0.16      0.16        19\n",
      "\n",
      "\n",
      "ðŸ”¹ NaÃ¯ve Bayes Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                                     CNNâ€™s Post-Debate Rigged Poll Bustedâ€¦Here Are The Real Results       0.00      0.00      0.00         0\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                                                    Website is Down For Maintenance       1.00      1.00      1.00         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.15      0.15      0.15        19\n",
      "                                                                                                       weighted avg       0.16      0.16      0.16        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Random Forest Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                           BOOM! Charlotte Officials FINALLY Silence Rioters With One E P I C Announcement! [VIDEO]       0.00      0.00      0.00         0\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       0.33      1.00      0.50         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                               Congress MUST Act! Five Days Until Obama's Internet Giveaway [Video]       0.00      0.00      0.00         0\n",
      "       DISGUSTING! Because Of Hillary & Obama, NY Terrorist Will Get Better Treatment Than US Vets! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "             NYC Terrorist Ahmad Rahami Sued Police Department for â€˜Religious Persecutionâ€™ in 2011! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "          No WAY! Liberal Judge Just Made A Law For Black Males To Do This DISGUSTING Thing To Cops â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                                          Note In Arabic Found With NYC Bombâ€¦ Call Says There Will Be More Bombings       0.00      0.00      0.00         0\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                                                                                  Obama Pushes One World Government       0.00      0.00      0.00         0\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                      The Pope Flat Out Called Fox News Type Journalism â€˜Terrorismâ€™       0.00      0.00      0.00         0\n",
      "             Trump Just Made A Campaign Promise So Ridiculous It Makes â€˜Read My Lipsâ€™ Look Good â€¹ Opposition Report       0.00      0.00      0.00         0\n",
      "                                                                                    Website is Down For Maintenance       0.20      1.00      0.33         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                                                â€˜We Donâ€™t Adapt To Your Culture, You Adapt To Oursâ€™       0.00      0.00      0.00         0\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.05      0.10      0.06        19\n",
      "                                                                                                       weighted avg       0.08      0.16      0.10        19\n",
      "\n",
      "\n",
      "ðŸ”¹ SVM Accuracy: 0.1579\n",
      "                                                                                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                                                        'Reporters' FLEE When Clintons Get EXPOSED!       0.00      0.00      0.00         1\n",
      "                                A Hillary Clinton Administration May be Entirely Run by a FIGUREHEAD â€“ Eagle Rising       1.00      1.00      1.00         1\n",
      "                                             Angela Merkel Admits Bringing In Muslim Refugees Was A Big Fat Mistake       0.00      0.00      0.00         1\n",
      "                                                             Australia Voted To Ban Muslims And Liberals Are Pissed       0.00      0.00      0.00         0\n",
      "                                                          BOOM! Merkel Admits Flooding Germany With Muslim Refugees       0.00      0.00      0.00         0\n",
      "                BREAKING: Steps to FORCE FBI Director Comey to Resign In Process â€“ Hearing Decides His Fate Sept 28       0.00      0.00      0.00         1\n",
      "                                          BREAKING: WikiLeaks Releases Proof Hillary Lied and The FBI Covered It Up       0.00      0.00      0.00         0\n",
      "                                                                Bill And Hillary Clinton Have A MAJOR AIDS Scandal!       0.00      0.00      0.00         0\n",
      "                             Bombing Suspect Filed Anti-Muslim Discrimination Lawsuit Against Police â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "                                        CNN FURIOUS and Embarrassed After Trump Called Them Out For One Major Thing       0.00      0.00      0.00         0\n",
      "                                Charlotte Thugs Leaves Copâ€™s Body With Disgusting Thing They Did... â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "              Clemson BANS Memes Featuring Dead Gorilla Harambeâ€¦Promotes â€œRape Cultureâ€ and â€œRacismâ€ â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "              Clinton Foundation Spent 5.7% on Charity; Rest Went to â€œSalariesâ€ and â€œOther Expensesâ€ â€“ Eagle Rising       0.50      1.00      0.67         1\n",
      "     Clinton's Exploited Haiti Earthquake â€˜to Steal Billions of Dollars from the Sick and Starvingâ€™ â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Donald Trump Goes Off The Deep Endâ€“May Be Unfit For Debate â€¹ Opposition Report       0.00      0.00      0.00         1\n",
      "                                         Fast Food CEO Threatens To Fire Everyone If A Democrat Wins The Presidency       0.00      0.00      0.00         1\n",
      "        Female Muslim-American Olympian Bashes U.S.A. â€”Goes Off The Hinges About Why America Sucks! â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                                 Hillary Denies She and Obama Founded ISISâ€¦Then This Video Shows Up       0.00      0.00      0.00         0\n",
      "                   Michelle Obama NOT Leaving The White House â€“ Hillary Clinton Has Terrifying New Role For Her!?!?       0.00      0.00      0.00         1\n",
      "             NYC Terrorist Ahmad Rahami Sued Police Department for â€˜Religious Persecutionâ€™ in 2011! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "          No WAY! Liberal Judge Just Made A Law For Black Males To Do This DISGUSTING Thing To Cops â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                          Obama Just DISGRACED The U.S. By Surrendering To ISIS In Front Of Entire U.N. â‹† US Herald       0.00      0.00      0.00         1\n",
      "                                                                                  Obama Pushes One World Government       0.00      0.00      0.00         0\n",
      "                                       Obamaâ€™s Speech In New York Shocks Americans When He Says THIS About Attacks!       0.00      0.00      0.00         0\n",
      "                      Proof The Mainstream Media Is Manipulating The Election By Taking Bill Clinton Out Of Context       0.00      0.00      0.00         1\n",
      "                             SURPRISE! 70% Of Charlotte RIOTERS Are Not From There â€“ Guess What That Meansâ€¦ [VIDEO]       0.00      0.00      0.00         0\n",
      "                                  Tebow DISMANTLES national anthem protests in one sentence...BOOM! â‹† Freedom Daily       0.00      0.00      0.00         0\n",
      "                                           The AP, In 2004, Said Your Boy Obama Was BORN In This Particular Country       0.00      0.00      0.00         1\n",
      "The NFL Told Russell Wilson Not To Speak About His Faith, So He Did Something Fans Couldn't Believe â‹† Freedom Daily       0.00      0.00      0.00         1\n",
      "                                     Top Army Chief of Staff General Has Had It With Obama Incompetence â‹† US Herald       0.00      0.00      0.00         0\n",
      "             Trump Just Made A Campaign Promise So Ridiculous It Makes â€˜Read My Lipsâ€™ Look Good â€¹ Opposition Report       0.00      0.00      0.00         0\n",
      "                                                   Trumpâ€™s Latest Campaign Promise May Be His Most Horrible One Yet       0.00      0.00      0.00         0\n",
      "                                                                                    Website is Down For Maintenance       1.00      1.00      1.00         1\n",
      "                                         Why is it â€œRACISTâ€ to Question Someoneâ€™s Birth Certificate? â€“ Eagle Rising       0.00      0.00      0.00         1\n",
      "\n",
      "                                                                                                           accuracy                           0.16        19\n",
      "                                                                                                          macro avg       0.07      0.09      0.08        19\n",
      "                                                                                                       weighted avg       0.13      0.16      0.14        19\n",
      "\n",
      "âœ… Model training and evaluation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ðŸ”¹ Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ðŸ”¹ Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ðŸ”¹ Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure input is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenize text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "        return \" \".join(words)\n",
    "    return text  # Return original non-string values as is\n",
    "\n",
    "# ðŸ”¹ Load dataset\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "df['text'] = df.select_dtypes(include=[\"object\"]).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# ðŸ”¹ Define target variable (assuming there is a 'label' column with 1 for Fake, 0 for Real)\n",
    "if 'title' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'label' column for classification.\")\n",
    "X = df['text']\n",
    "y = df['title']\n",
    "\n",
    "# âœ… **Apply TF-IDF Vectorization**\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# ðŸ”¹ Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… **Train and Evaluate Models**\n",
    "def train_and_evaluate(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nðŸ”¹ {model_name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"NaÃ¯ve Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    train_and_evaluate(model, name)\n",
    "\n",
    "print(\"âœ… Model training and evaluation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c1dc23-dbd9-4173-933d-05dac910f7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, TFBertForSequenceClassification\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1ï¸âƒ£ Load Dataset\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")  # Update file name if needed\n",
    "df = df.dropna(subset=[\"text\", \"title\"])  # Remove missing values\n",
    "\n",
    "# 2ï¸âƒ£ Define Features (X) and Target (y)\n",
    "X = df[\"text\"].values  # Using news text as input\n",
    "y = df[\"title\"].values  # 1 = Fake, 0 = Real\n",
    "\n",
    "# 3ï¸âƒ£ Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------------------\n",
    "# âœ… Approach 1: LSTM Model\n",
    "# -------------------------------------\n",
    "\n",
    "# 4ï¸âƒ£ Tokenization & Padding\n",
    "MAX_NUM_WORDS = 5000  # Vocabulary size\n",
    "MAX_SEQ_LENGTH = 500  # Max words per article\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_SEQ_LENGTH)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "# 5ï¸âƒ£ Build LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(MAX_NUM_WORDS, 128, input_length=MAX_SEQ_LENGTH),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "# 6ï¸âƒ£ Train LSTM Model\n",
    "lstm_model.fit(X_train_seq, y_train, validation_data=(X_test_seq, y_test), epochs=5, batch_size=64)\n",
    "\n",
    "# 7ï¸âƒ£ Evaluate LSTM Model\n",
    "y_pred_lstm = (lstm_model.predict(X_test_seq) > 0.5).astype(\"int32\")\n",
    "print(\"ðŸ”¹ LSTM Model Performance:\\n\", classification_report(y_test, y_pred_lstm))\n",
    "\n",
    "# -------------------------------------\n",
    "# âœ… Approach 2: BERT Model\n",
    "# -------------------------------------\n",
    "\n",
    "# 8ï¸âƒ£ Tokenization with BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_len=512):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors=\"tf\")\n",
    "\n",
    "X_train_bert = encode_texts(X_train, bert_tokenizer)\n",
    "X_test_bert = encode_texts(X_test, bert_tokenizer)\n",
    "\n",
    "# 9ï¸âƒ£ Load Pretrained BERT Model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 1ï¸âƒ£0ï¸âƒ£ Compile & Train BERT Model\n",
    "bert_model.compile(optimizer=Adam(learning_rate=2e-5), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "bert_model.fit(X_train_bert.data, y_train, validation_data=(X_test_bert.data, y_test), epochs=3, batch_size=16)\n",
    "\n",
    "# 1ï¸âƒ£1ï¸âƒ£ Evaluate BERT Model\n",
    "y_pred_bert = np.argmax(bert_model.predict(X_test_bert.data).logits, axis=1)\n",
    "print(\"ðŸ”¹ BERT Model Performance:\\n\", classification_report(y_test, y_pred_bert))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1a6bb-7404-447a-9a45-7965cf90a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6a59f-7cfd-4458-b29b-3281c65f00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2cba93-f75b-457b-abbf-dc1002de8e08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Should print the installed TensorFlow version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791f90-bf6a-4506-a68d-1a7ace1d2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021378e-ba74-4ad6-ba5e-e9000301f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdc4838-3694-4c19-9d76-d139d1b6c74a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c856a8-a752-4806-bf67-5f100818913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf98c741-f66d-410f-bd23-d36168767d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 653.7 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 397.0 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 397.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 443.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 483.6 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634dddf0-2114-45f7-8a30-bc3e5d066d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777b22e5-1035-4824-b21e-6e291719ac7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m python \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39mversion\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3ad625-84c8-42a1-98d3-70bc9426e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "pip 25.0.1 from C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\pip (python 3.12)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "!pip --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60536f0-8cd9-40e0-ac30-a587e53df050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (25.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a22a2-230d-4a0b-952c-0f2f4e66c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f82ab3-6438-418e-a5f6-76cc50564e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be8e0e6-99f2-410b-8e37-de9526b01beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de5f71d-11e1-4ea8-a582-4e20fc4358de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "def main():\n",
    "    st.title(\"CSV File Preview Tool\")\n",
    "    st.write(\"Upload a CSV file to preview its contents.\")\n",
    "    \n",
    "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=[\"csv\"])\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        df = pd.read_csv(uploaded_file)\n",
    "        num_lines = st.slider(\"Select number of lines to preview\", min_value=1, max_value=20, value=5)\n",
    "        st.write(df.head(num_lines))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d81b75-be9d-41c4-91b2-068e9bc4b220",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "df['text'] = df.select_dtypes(include=[\"object\"]).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "if 'title' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'title' column for classification.\")\n",
    "X = df['text']\n",
    "y = df['title']\n",
    "\n",
    "# Convert text to sequences for LSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_seq, maxlen=200)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_lstm = (lstm_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"\\nLSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(classification_report(y_test, y_pred_lstm))\n",
    "\n",
    "# Apply BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_text(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "\n",
    "tokenized_train = tokenize_text(X_train)\n",
    "tokenized_test = tokenize_text(X_test)\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "bert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "bert_model.fit(tokenized_train, y_train, epochs=3, batch_size=8, validation_data=(tokenized_test, y_test))\n",
    "\n",
    "# Evaluate BERT model\n",
    "y_pred_bert = np.argmax(bert_model.predict(tokenized_test).logits, axis=1)\n",
    "print(\"\\nBERT Accuracy:\", accuracy_score(y_test, y_pred_bert))\n",
    "print(classification_report(y_test, y_pred_bert))\n",
    "\n",
    "print(\"âœ… Deep learning models (LSTM & BERT) completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d64b24-4eb3-495b-bb37-8a2049d71897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c893d4-3617-40ec-aa49-188517896fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c144842d-5173-4df5-96a9-3c6d44f9083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6e0028-8a23-48bb-870f-e8ef4ef0c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 69\u001b[0m\n\u001b[0;32m     60\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     61\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m),\n\u001b[0;32m     62\u001b[0m     LSTM(\u001b[38;5;241m128\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     66\u001b[0m ])\n\u001b[0;32m     68\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 69\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Evaluate LSTM model\u001b[39;00m\n\u001b[0;32m     72\u001b[0m y_pred_lstm \u001b[38;5;241m=\u001b[39m (lstm_model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optree\\ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treespec\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;28mmap\u001b[39m(func, \u001b[38;5;241m*\u001b[39mflat_args))\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/Dell/Desktop/NLP-PROJECT/archive/BuzzFeed_fake_news_content.csv\")\n",
    "\n",
    "df['text'] = df.select_dtypes(include=[\"object\"]).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "if 'title' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'title' column for classification.\")\n",
    "X = df['text']\n",
    "y = df['title']\n",
    "\n",
    "# Convert text to sequences for LSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_seq, maxlen=200)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_lstm = (lstm_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"\\nLSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(classification_report(y_test, y_pred_lstm))\n",
    "\n",
    "# Apply BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_text(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "\n",
    "tokenized_train = tokenize_text(X_train)\n",
    "tokenized_test = tokenize_text(X_test)\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "bert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "bert_model.fit(tokenized_train, y_train, epochs=3, batch_size=8, validation_data=(tokenized_test, y_test))\n",
    "\n",
    "# Evaluate BERT model\n",
    "y_pred_bert = np.argmax(bert_model.predict(tokenized_test).logits, axis=1)\n",
    "print(\"\\nBERT Accuracy:\", accuracy_score(y_test, y_pred_bert))\n",
    "print(classification_report(y_test, y_pred_bert))\n",
    "\n",
    "print(\"âœ… Deep learning models (LSTM & BERT) completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c100fa-7cd9-4050-b04f-b323cc208592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9d3053-b6ba-4c5b-942e-29e2d349815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "\n",
      "ðŸ“ Dataset Size: (4, 5)\n",
      "\n",
      "ðŸ“Š Preview of Scraped Dataset:\n",
      "                                               Title   Author        Date  \\\n",
      "0  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿...  Unknown  2020-08-01   \n",
      "1  Mechanism to detect fake news on health developed  Authors  2020-07-31   \n",
      "2  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿...  Unknown  2020-08-01   \n",
      "3  Mechanism to detect fake news on health developed  Authors  2020-07-31   \n",
      "\n",
      "                                             Content  \\\n",
      "0  Click on â€˜Get News Alertsâ€™ to get the latest n...   \n",
      "1  Researchers at the University of Calicut in co...   \n",
      "2  Click on â€˜Get News Alertsâ€™ to get the latest n...   \n",
      "3  Researchers at the University of Calicut in co...   \n",
      "\n",
      "                                              Source  \n",
      "0  https://www.mathrubhumi.com/technology/news/ar...  \n",
      "1  https://www.thehindu.com/news/cities/kozhikode...  \n",
      "2  https://www.mathrubhumi.com/technology/news/ar...  \n",
      "3  https://www.thehindu.com/news/cities/kozhikode...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "# âœ… Use a mix of real and fake news sources (2â€“3 total for demo/testing)\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# Function to collect article URLs\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))  # Remove duplicates\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract article details using Newspaper3k\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Scrape and collect articles\n",
    "articles_data = []\n",
    "\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    \n",
    "    for link in links[:30]:  # Limit to 30 articles per site\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Convert to DataFrame and preview\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "print(f\"\\nðŸ“ Dataset Size: {df.shape}\")\n",
    "print(\"\\nðŸ“Š Preview of Scraped Dataset:\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec11096-dfcb-4e77-b300-aacd45ad3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?utm_source=chatgpt.com\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "\n",
      "ðŸ“ Dataset Size: (2, 5)\n",
      "\n",
      "ðŸ“Š Full Dataset Preview:\n",
      "\n",
      "                                                                   Title  \\\n",
      "0  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚   \n",
      "1                      Mechanism to detect fake news on health developed   \n",
      "\n",
      "    Author        Date  \\\n",
      "0  Unknown  2020-08-01   \n",
      "1  Authors  2020-07-31   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Content  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Click on â€˜Get News Alertsâ€™ to get the latest news alerts from\\n\\nNot now Get News Alerts   \n",
      "1  Researchers at the University of Calicut in collaboration with Queenâ€™s University, Belfast, United Kingdom, have developed an emotion cognizant health fake news detection mechanism to curtail disinformation and misinformation in the wake of the COVID-19 scare.\\n\\nFor the last two years, experts at the two universities have been working towards tackling fake news on health. Now, The requirement is more after the World Health Organisation (WHO) cautioned against giving false hopes to people exploiting the widespread fear of the pandemic.\\n\\nDr. Lajish V.L., head of the department of computer science, University of Calicut, said in a statement on Friday that the research method relied on an exceedingly simple mechanism, that of identifying emotionally oriented words in a news article and adding the emotion label beside them, before using detection mechanisms.\\n\\nArtificial Intelligence\\n\\nâ€œWe, at the Computational Intelligence and Data Analytics [CIDA] Lab, started our work in this area by understanding the holistic approach of this research. We used different types of advanced Machine Learning and Artificial Intelligence algorithms, especially to cultivate emotion amplification-based text representations and to conduct fake news detection experiments,â€ he said.\\n\\nExplaining the need for a high-potential fake news detection system, K. Anoop, a researcher, said the commercial and political phenomenon of empathically optimised automated fake news was on near-horizon. His research on effect-oriented fake news detection using Machine Learning was one among the AWSAR-2019 award winners for the best popular science stories, instituted by the Department of Science and Technology.\\n\\nâ€œThe simplicity of the emotion-enrichment method we propose makes it applicable for usage within a diversity of fake news detection tools,â€ said Dr. Deepak Padmanabhan, faculty member of the computer science department at Queenâ€™s University. In view of COVID-19 regulations that make travel impossible, the researchers will present their work virtually to the research community. The department of computer science, University of Calicut, has set up a web portal â€” https://dcs.uoc.ac.in/cida/break-the-fake â€” for collecting fake news in both English and Malayalam from the public for experimentation.\\n\\nThe portal will eventually evolve into a fact-finding system which will be freely available in the public domain.   \n",
      "\n",
      "                                                                                                                                                                                                       Source  \n",
      "0  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac  \n",
      "1                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "# âœ… Use a mix of real and fake news sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?utm_source=chatgpt.com\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# Function to collect article URLs\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))  # Remove duplicates\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract article details\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Collect articles\n",
    "articles_data = []\n",
    "\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    \n",
    "    for link in links[:30]:  # Up to 30 per source\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)  # Be nice to the server\n",
    "\n",
    "# âœ… Create DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# ðŸ‘‰ Force full DataFrame printing\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# âœ… Print dataset shape and full contents\n",
    "print(f\"\\nðŸ“ Dataset Size: {df.shape}\")\n",
    "print(\"\\nðŸ“Š Full Dataset Preview:\\n\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929cdde5-2424-43d3-aac0-8a8afa56d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "\n",
      "ðŸ“ Dataset Size: (4, 5)\n",
      "\n",
      "ðŸ“Š Full Scraped Dataset:\n",
      "                                                                   Title   Author        Date                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Content                                                                                                                                                                                                      Source\n",
      "0  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown  2020-08-01                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Click on â€˜Get News Alertsâ€™ to get the latest news alerts from\\n\\nNot now Get News Alerts  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "1                      Mechanism to detect fake news on health developed  Authors  2020-07-31  Researchers at the University of Calicut in collaboration with Queenâ€™s University, Belfast, United Kingdom, have developed an emotion cognizant health fake news detection mechanism to curtail disinformation and misinformation in the wake of the COVID-19 scare.\\n\\nFor the last two years, experts at the two universities have been working towards tackling fake news on health. Now, The requirement is more after the World Health Organisation (WHO) cautioned against giving false hopes to people exploiting the widespread fear of the pandemic.\\n\\nDr. Lajish V.L., head of the department of computer science, University of Calicut, said in a statement on Friday that the research method relied on an exceedingly simple mechanism, that of identifying emotionally oriented words in a news article and adding the emotion label beside them, before using detection mechanisms.\\n\\nArtificial Intelligence\\n\\nâ€œWe, at the Computational Intelligence and Data Analytics [CIDA] Lab, started our work in this area by understanding the holistic approach of this research. We used different types of advanced Machine Learning and Artificial Intelligence algorithms, especially to cultivate emotion amplification-based text representations and to conduct fake news detection experiments,â€ he said.\\n\\nExplaining the need for a high-potential fake news detection system, K. Anoop, a researcher, said the commercial and political phenomenon of empathically optimised automated fake news was on near-horizon. His research on effect-oriented fake news detection using Machine Learning was one among the AWSAR-2019 award winners for the best popular science stories, instituted by the Department of Science and Technology.\\n\\nâ€œThe simplicity of the emotion-enrichment method we propose makes it applicable for usage within a diversity of fake news detection tools,â€ said Dr. Deepak Padmanabhan, faculty member of the computer science department at Queenâ€™s University. In view of COVID-19 regulations that make travel impossible, the researchers will present their work virtually to the research community. The department of computer science, University of Calicut, has set up a web portal â€” https://dcs.uoc.ac.in/cida/break-the-fake â€” for collecting fake news in both English and Malayalam from the public for experimentation.\\n\\nThe portal will eventually evolve into a fact-finding system which will be freely available in the public domain.                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "2  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown  2020-08-01                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Click on â€˜Get News Alertsâ€™ to get the latest news alerts from\\n\\nNot now Get News Alerts  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "3                      Mechanism to detect fake news on health developed  Authors  2020-07-31  Researchers at the University of Calicut in collaboration with Queenâ€™s University, Belfast, United Kingdom, have developed an emotion cognizant health fake news detection mechanism to curtail disinformation and misinformation in the wake of the COVID-19 scare.\\n\\nFor the last two years, experts at the two universities have been working towards tackling fake news on health. Now, The requirement is more after the World Health Organisation (WHO) cautioned against giving false hopes to people exploiting the widespread fear of the pandemic.\\n\\nDr. Lajish V.L., head of the department of computer science, University of Calicut, said in a statement on Friday that the research method relied on an exceedingly simple mechanism, that of identifying emotionally oriented words in a news article and adding the emotion label beside them, before using detection mechanisms.\\n\\nArtificial Intelligence\\n\\nâ€œWe, at the Computational Intelligence and Data Analytics [CIDA] Lab, started our work in this area by understanding the holistic approach of this research. We used different types of advanced Machine Learning and Artificial Intelligence algorithms, especially to cultivate emotion amplification-based text representations and to conduct fake news detection experiments,â€ he said.\\n\\nExplaining the need for a high-potential fake news detection system, K. Anoop, a researcher, said the commercial and political phenomenon of empathically optimised automated fake news was on near-horizon. His research on effect-oriented fake news detection using Machine Learning was one among the AWSAR-2019 award winners for the best popular science stories, instituted by the Department of Science and Technology.\\n\\nâ€œThe simplicity of the emotion-enrichment method we propose makes it applicable for usage within a diversity of fake news detection tools,â€ said Dr. Deepak Padmanabhan, faculty member of the computer science department at Queenâ€™s University. In view of COVID-19 regulations that make travel impossible, the researchers will present their work virtually to the research community. The department of computer science, University of Calicut, has set up a web portal â€” https://dcs.uoc.ac.in/cida/break-the-fake â€” for collecting fake news in both English and Malayalam from the public for experimentation.\\n\\nThe portal will eventually evolve into a fact-finding system which will be freely available in the public domain.                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "# âœ… Use a mix of real and fake news sources (2â€“3 total for demo/testing)\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# Function to collect article URLs\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))  # Remove duplicates\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract article details using Newspaper3k\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Scrape and collect articles\n",
    "articles_data = []\n",
    "\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    \n",
    "    for link in links[:300]:  # Limit to 30 articles per site\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Convert to DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Set pandas options to print all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# âœ… Print full dataset\n",
    "print(f\"\\nðŸ“ Dataset Size: {df.shape}\")\n",
    "print(\"\\nðŸ“Š Full Scraped Dataset:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c65fdb3-b313-4e6c-a5e6-fa9cf21cb222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "\n",
      "ðŸ“ Dataset Size: (4, 6)\n",
      "\n",
      "ðŸ“Š Full Scraped and Preprocessed Dataset:\n",
      "                                                                   Title   Author        Date                                                                                                                                                                                                      Source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Cleaned_Content\n",
      "0  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown  2020-08-01  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           click get news alerts get latest news alerts get news alerts\n",
      "1                      Mechanism to detect fake news on health developed  Authors  2020-07-31                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece  researchers university calicut collaboration queens university belfast united kingdom developed emotion cognizant health fake news detection mechanism curtail disinformation misinformation wake covid19 scare last two years experts two universities working towards tackling fake news health requirement world health organisation cautioned giving false hopes people exploiting widespread fear pandemic dr lajish vl head department computer science university calicut said statement friday research method relied exceedingly simple mechanism identifying emotionally oriented words news article adding emotion label beside using detection mechanisms artificial intelligence computational intelligence data analytics cida lab started work area understanding holistic approach research used different types advanced machine learning artificial intelligence algorithms especially cultivate emotion amplificationbased text representations conduct fake news detection experiments said explaining need highpotential fake news detection system k anoop researcher said commercial political phenomenon empathically optimised automated fake news nearhorizon research effectoriented fake news detection using machine learning one among awsar2019 award winners best popular science stories instituted department science technology simplicity emotionenrichment method propose makes applicable usage within diversity fake news detection tools said dr deepak padmanabhan faculty member computer science department queens university view covid19 regulations make travel impossible researchers present work virtually research community department computer science university calicut set web portal httpsdcsuocacincidabreakthefake collecting fake news english malayalam public experimentation portal eventually evolve factfinding system freely available public domain\n",
      "2  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown  2020-08-01  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           click get news alerts get latest news alerts get news alerts\n",
      "3                      Mechanism to detect fake news on health developed  Authors  2020-07-31                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece  researchers university calicut collaboration queens university belfast united kingdom developed emotion cognizant health fake news detection mechanism curtail disinformation misinformation wake covid19 scare last two years experts two universities working towards tackling fake news health requirement world health organisation cautioned giving false hopes people exploiting widespread fear pandemic dr lajish vl head department computer science university calicut said statement friday research method relied exceedingly simple mechanism identifying emotionally oriented words news article adding emotion label beside using detection mechanisms artificial intelligence computational intelligence data analytics cida lab started work area understanding holistic approach research used different types advanced machine learning artificial intelligence algorithms especially cultivate emotion amplificationbased text representations conduct fake news detection experiments said explaining need highpotential fake news detection system k anoop researcher said commercial political phenomenon empathically optimised automated fake news nearhorizon research effectoriented fake news detection using machine learning one among awsar2019 award winners best popular science stories instituted department science technology simplicity emotionenrichment method propose makes applicable usage within diversity fake news detection tools said dr deepak padmanabhan faculty member computer science department queens university view covid19 regulations make travel impossible researchers present work virtually research community department computer science university calicut set web portal httpsdcsuocacincidabreakthefake collecting fake news english malayalam public experimentation portal eventually evolve factfinding system freely available public domain\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# âœ… Download NLTK data (only needs to run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… Use a mix of real and fake news sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# âœ… Collect article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Reconstruct cleaned text\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# âœ… Scrape articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:300]:\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Load into DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Preprocess the content\n",
    "df[\"Cleaned_Content\"] = df[\"Content\"].apply(preprocess_text)\n",
    "\n",
    "# âœ… Set display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# âœ… Output\n",
    "print(f\"\\nðŸ“ Dataset Size: {df.shape}\")\n",
    "print(\"\\nðŸ“Š Full Scraped and Preprocessed Dataset:\")\n",
    "print(df[[\"Title\", \"Author\", \"Date\", \"Source\", \"Cleaned_Content\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b9dd6a9-294c-4efb-8d91-43987ed27466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "\n",
      "ðŸ”  TF-IDF Feature Shape: (4, 158)\n",
      "\n",
      "ðŸ“ Dataset Size: (4, 8)\n",
      "\n",
      "ðŸ“Š Sample of Preprocessed DataFrame with Features:\n",
      "                                                                   Title   Author                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Cleaned_Content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        W2V_Vector\n",
      "0  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           click get news alerts get latest news alerts get news alerts                                                           [-0.0005337538, 0.004443659, -0.001981547, 0.0005952604, -0.002584743, -0.003156393, 0.001097879, 0.002320209, -5.486341e-05, -0.0031501243, 0.0018662573, -0.0013076365, -0.004311207, -0.0034633682, -0.0005714543, 0.0028658367, 0.0059443726, -0.00016758028, -0.001617616, -0.006267531, 0.006076278, 0.004176729, 0.0039775325, 0.00017327337, 0.001756093, 0.0025124503, -0.0023109093, -0.0014762335, 0.0002477539, -0.00081031804, -0.00231915, 0.00041098165, 0.0051341224, -0.0051296316, -0.0007371049, -0.0031272853, 0.001745389, 0.0024721401, 0.00079384627, -0.00014946093, -0.0022089472, -0.00035950504, 0.001546052, 0.0017854505, 0.000712773, 0.0012939627, -0.00592934, 0.0010036568, 0.002030153, 0.0016961037, -0.0013084762, 0.0019084006, -0.0027029999, -0.0017806441, 0.0028415844, -0.0026234305, 0.0054042917, -0.0021216448, -0.001653554, 0.0053791483, -0.0008108039, -0.00038983178, 0.0008849058, 0.00167947, 0.001922138, 0.001498261, 0.00082493667, 0.0024863596, 0.00014131227, -0.00049279496, -0.001901447, -0.00083393214, -0.00030420863, -0.00215735, 0.0024671864, 0.00021283743, 0.004675065, 0.00046784955, -0.000101589874, -0.0035012665, -0.0010062368, 0.0012469019, 0.0037355346, 0.000710306, 0.00013367558, 0.0023761266, 0.0053004534, 0.0024673126, -0.0017154864, -0.0025537016, 0.004016088, 0.0031162724, 0.0034194014, 0.0023442877, 0.0021896667, -0.0011063262, 0.0003176307, -0.0005334683, -0.002697737, 0.0071772817]\n",
      "1                      Mechanism to detect fake news on health developed  Authors  researchers university calicut collaboration queens university belfast united kingdom developed emotion cognizant health fake news detection mechanism curtail disinformation misinformation wake covid19 scare last two years experts two universities working towards tackling fake news health requirement world health organisation cautioned giving false hopes people exploiting widespread fear pandemic dr lajish vl head department computer science university calicut said statement friday research method relied exceedingly simple mechanism identifying emotionally oriented words news article adding emotion label beside using detection mechanisms artificial intelligence computational intelligence data analytics cida lab started work area understanding holistic approach research used different types advanced machine learning artificial intelligence algorithms especially cultivate emotion amplificationbased text representations conduct fake news detection experiments said explaining need highpotential fake news detection system k anoop researcher said commercial political phenomenon empathically optimised automated fake news nearhorizon research effectoriented fake news detection using machine learning one among awsar2019 award winners best popular science stories instituted department science technology simplicity emotionenrichment method propose makes applicable usage within diversity fake news detection tools said dr deepak padmanabhan faculty member computer science department queens university view covid19 regulations make travel impossible researchers present work virtually research community department computer science university calicut set web portal httpsdcsuocacincidabreakthefake collecting fake news english malayalam public experimentation portal eventually evolve factfinding system freely available public domain  [-0.00040738215, 0.00046388234, 0.0003502298, 0.0004940808, 0.00020060426, -0.0013837753, 0.0011751348, 0.0020992141, -0.0015360097, -0.0006355314, -0.00011529175, -0.001266799, -0.00011272234, 0.0011989279, 0.00016238884, -0.00030040595, 0.0006894172, -0.000401303, -0.0009213793, -0.002090791, 0.00040329937, -2.2631051e-05, 0.00059059786, -0.00059024605, -0.00014076216, -8.240697e-05, -0.0003498518, 0.0002620314, -0.0009696336, 0.00020388991, 0.0011646355, -0.0005846486, 0.00013939153, -0.0011428326, -0.00032750078, 0.0012900889, 0.0007689896, -0.0004721447, -0.00020133496, -0.0009357651, 0.00025968632, -0.00058691495, -0.0011307993, 0.00044252592, 0.00080185593, -8.45183e-05, -0.00058575656, 6.941058e-05, 0.0006231109, 0.0004895478, 0.00030629127, -0.0008598859, 0.000111741894, 0.00014723124, -0.00031570467, 0.00041493805, 0.00019860227, 0.00014980051, -0.00061156176, 0.00077215483, 0.00017544764, -5.636185e-05, 0.00084710086, -0.000422767, -0.0010137113, 0.0014227496, 0.0005614465, 0.0010773735, -0.0011710264, 0.0010505129, -0.00026633032, 0.0008974527, 0.00062359683, -9.811082e-05, 0.00081322936, 0.00041411418, -0.00010609134, 0.00041415505, -0.00080278394, -0.00016051205, -0.0008357906, 0.0006892853, -0.0005619345, 0.0014185333, -0.00069981714, -0.00021869592, 0.0010645931, 0.0002527558, 0.00083516433, 0.00018166042, 0.0007482896, 0.0006237983, 0.00020635696, 4.0495535e-05, 0.0017319897, 0.0004448456, 0.0007776138, -0.0010269404, 0.0006302107, -4.0121417e-05]\n",
      "2  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           click get news alerts get latest news alerts get news alerts                                                           [-0.0005337538, 0.004443659, -0.001981547, 0.0005952604, -0.002584743, -0.003156393, 0.001097879, 0.002320209, -5.486341e-05, -0.0031501243, 0.0018662573, -0.0013076365, -0.004311207, -0.0034633682, -0.0005714543, 0.0028658367, 0.0059443726, -0.00016758028, -0.001617616, -0.006267531, 0.006076278, 0.004176729, 0.0039775325, 0.00017327337, 0.001756093, 0.0025124503, -0.0023109093, -0.0014762335, 0.0002477539, -0.00081031804, -0.00231915, 0.00041098165, 0.0051341224, -0.0051296316, -0.0007371049, -0.0031272853, 0.001745389, 0.0024721401, 0.00079384627, -0.00014946093, -0.0022089472, -0.00035950504, 0.001546052, 0.0017854505, 0.000712773, 0.0012939627, -0.00592934, 0.0010036568, 0.002030153, 0.0016961037, -0.0013084762, 0.0019084006, -0.0027029999, -0.0017806441, 0.0028415844, -0.0026234305, 0.0054042917, -0.0021216448, -0.001653554, 0.0053791483, -0.0008108039, -0.00038983178, 0.0008849058, 0.00167947, 0.001922138, 0.001498261, 0.00082493667, 0.0024863596, 0.00014131227, -0.00049279496, -0.001901447, -0.00083393214, -0.00030420863, -0.00215735, 0.0024671864, 0.00021283743, 0.004675065, 0.00046784955, -0.000101589874, -0.0035012665, -0.0010062368, 0.0012469019, 0.0037355346, 0.000710306, 0.00013367558, 0.0023761266, 0.0053004534, 0.0024673126, -0.0017154864, -0.0025537016, 0.004016088, 0.0031162724, 0.0034194014, 0.0023442877, 0.0021896667, -0.0011063262, 0.0003176307, -0.0005334683, -0.002697737, 0.0071772817]\n",
      "3                      Mechanism to detect fake news on health developed  Authors  researchers university calicut collaboration queens university belfast united kingdom developed emotion cognizant health fake news detection mechanism curtail disinformation misinformation wake covid19 scare last two years experts two universities working towards tackling fake news health requirement world health organisation cautioned giving false hopes people exploiting widespread fear pandemic dr lajish vl head department computer science university calicut said statement friday research method relied exceedingly simple mechanism identifying emotionally oriented words news article adding emotion label beside using detection mechanisms artificial intelligence computational intelligence data analytics cida lab started work area understanding holistic approach research used different types advanced machine learning artificial intelligence algorithms especially cultivate emotion amplificationbased text representations conduct fake news detection experiments said explaining need highpotential fake news detection system k anoop researcher said commercial political phenomenon empathically optimised automated fake news nearhorizon research effectoriented fake news detection using machine learning one among awsar2019 award winners best popular science stories instituted department science technology simplicity emotionenrichment method propose makes applicable usage within diversity fake news detection tools said dr deepak padmanabhan faculty member computer science department queens university view covid19 regulations make travel impossible researchers present work virtually research community department computer science university calicut set web portal httpsdcsuocacincidabreakthefake collecting fake news english malayalam public experimentation portal eventually evolve factfinding system freely available public domain  [-0.00040738215, 0.00046388234, 0.0003502298, 0.0004940808, 0.00020060426, -0.0013837753, 0.0011751348, 0.0020992141, -0.0015360097, -0.0006355314, -0.00011529175, -0.001266799, -0.00011272234, 0.0011989279, 0.00016238884, -0.00030040595, 0.0006894172, -0.000401303, -0.0009213793, -0.002090791, 0.00040329937, -2.2631051e-05, 0.00059059786, -0.00059024605, -0.00014076216, -8.240697e-05, -0.0003498518, 0.0002620314, -0.0009696336, 0.00020388991, 0.0011646355, -0.0005846486, 0.00013939153, -0.0011428326, -0.00032750078, 0.0012900889, 0.0007689896, -0.0004721447, -0.00020133496, -0.0009357651, 0.00025968632, -0.00058691495, -0.0011307993, 0.00044252592, 0.00080185593, -8.45183e-05, -0.00058575656, 6.941058e-05, 0.0006231109, 0.0004895478, 0.00030629127, -0.0008598859, 0.000111741894, 0.00014723124, -0.00031570467, 0.00041493805, 0.00019860227, 0.00014980051, -0.00061156176, 0.00077215483, 0.00017544764, -5.636185e-05, 0.00084710086, -0.000422767, -0.0010137113, 0.0014227496, 0.0005614465, 0.0010773735, -0.0011710264, 0.0010505129, -0.00026633032, 0.0008974527, 0.00062359683, -9.811082e-05, 0.00081322936, 0.00041411418, -0.00010609134, 0.00041415505, -0.00080278394, -0.00016051205, -0.0008357906, 0.0006892853, -0.0005619345, 0.0014185333, -0.00069981714, -0.00021869592, 0.0010645931, 0.0002527558, 0.00083516433, 0.00018166042, 0.0007482896, 0.0006237983, 0.00020635696, 4.0495535e-05, 0.0017319897, 0.0004448456, 0.0007776138, -0.0010269404, 0.0006302107, -4.0121417e-05]\n",
      "\n",
      "ðŸ“Š Sample TF-IDF DataFrame:\n",
      "   adding  advanced    alerts  algorithms   among  amplificationbased  analytics   anoop  applicable  approach    area  article  artificial  automated  available   award  awsar2019  belfast  beside    best   calicut  cautioned    cida     click  cognizant  collaboration  collecting  commercial  community  computational  computer  conduct   covid19  cultivate  curtail    data  deepak  department  detection  developed  different  disinformation  diversity  domain        dr  effectoriented   emotion  emotionally  emotionenrichment  empathically  english  especially  eventually  evolve  exceedingly  experimentation  experiments  experts  explaining  exploiting  factfinding  faculty      fake   false    fear  freely  friday       get  giving    head    health  highpotential  holistic   hopes  httpsdcsuocacincidabreakthefake  identifying  impossible  instituted  intelligence  kingdom     lab   label  lajish    last    latest  learning   machine    make   makes  malayalam  mechanism  mechanisms  member    method  misinformation  nearhorizon    need      news     one  optimised  organisation  oriented  padmanabhan  pandemic  people  phenomenon  political  popular    portal  present  propose    public    queens  regulations  relied  representations  requirement  research  researcher  researchers      said   scare   science     set  simple  simplicity  started  statement  stories    system  tackling  technology    text   tools  towards  travel       two   types  understanding  united  universities  university   usage    used     using    view  virtually      vl    wake     web  widespread  winners  within   words      work  working   world   years\n",
      "0  0.0000    0.0000  0.613102      0.0000  0.0000              0.0000     0.0000  0.0000      0.0000    0.0000  0.0000   0.0000    0.000000     0.0000     0.0000  0.0000     0.0000   0.0000  0.0000  0.0000  0.000000     0.0000  0.0000  0.204367     0.0000         0.0000      0.0000      0.0000     0.0000         0.0000  0.000000   0.0000  0.000000     0.0000   0.0000  0.0000  0.0000    0.000000   0.000000     0.0000     0.0000          0.0000     0.0000  0.0000  0.000000          0.0000  0.000000       0.0000             0.0000        0.0000   0.0000      0.0000      0.0000  0.0000       0.0000           0.0000       0.0000   0.0000      0.0000      0.0000       0.0000   0.0000  0.000000  0.0000  0.0000  0.0000  0.0000  0.613102  0.0000  0.0000  0.000000         0.0000    0.0000  0.0000                           0.0000       0.0000      0.0000      0.0000      0.000000   0.0000  0.0000  0.0000  0.0000  0.0000  0.204367  0.000000  0.000000  0.0000  0.0000     0.0000   0.000000      0.0000  0.0000  0.000000          0.0000       0.0000  0.0000  0.405806  0.0000     0.0000        0.0000    0.0000       0.0000    0.0000  0.0000      0.0000     0.0000   0.0000  0.000000   0.0000   0.0000  0.000000  0.000000       0.0000  0.0000           0.0000       0.0000  0.000000      0.0000     0.000000  0.000000  0.0000  0.000000  0.0000  0.0000      0.0000   0.0000     0.0000   0.0000  0.000000    0.0000      0.0000  0.0000  0.0000   0.0000  0.0000  0.000000  0.0000         0.0000  0.0000        0.0000    0.000000  0.0000  0.0000  0.000000  0.0000     0.0000  0.0000  0.0000  0.0000      0.0000   0.0000  0.0000  0.0000  0.000000   0.0000  0.0000  0.0000\n",
      "1  0.0464    0.0464  0.000000      0.0464  0.0464              0.0464     0.0464  0.0464      0.0464    0.0464  0.0464   0.0464    0.092799     0.0464     0.0464  0.0464     0.0464   0.0464  0.0464  0.0464  0.139199     0.0464  0.0464  0.000000     0.0464         0.0464      0.0464      0.0464     0.0464         0.0464  0.139199   0.0464  0.092799     0.0464   0.0464  0.0464  0.0464    0.185598   0.278397     0.0464     0.0464          0.0464     0.0464  0.0464  0.092799          0.0464  0.139199       0.0464             0.0464        0.0464   0.0464      0.0464      0.0464  0.0464       0.0464           0.0464       0.0464   0.0464      0.0464      0.0464       0.0464   0.0464  0.371196  0.0464  0.0464  0.0464  0.0464  0.000000  0.0464  0.0464  0.139199         0.0464    0.0464  0.0464                           0.0464       0.0464      0.0464      0.0464      0.139199   0.0464  0.0464  0.0464  0.0464  0.0464  0.000000  0.092799  0.092799  0.0464  0.0464     0.0464   0.092799      0.0464  0.0464  0.092799          0.0464       0.0464  0.0464  0.276402  0.0464     0.0464        0.0464    0.0464       0.0464    0.0464  0.0464      0.0464     0.0464   0.0464  0.092799   0.0464   0.0464  0.092799  0.092799       0.0464  0.0464           0.0464       0.0464  0.185598      0.0464     0.092799  0.185598  0.0464  0.231998  0.0464  0.0464      0.0464   0.0464     0.0464   0.0464  0.092799    0.0464      0.0464  0.0464  0.0464   0.0464  0.0464  0.092799  0.0464         0.0464  0.0464        0.0464    0.231998  0.0464  0.0464  0.092799  0.0464     0.0464  0.0464  0.0464  0.0464      0.0464   0.0464  0.0464  0.0464  0.092799   0.0464  0.0464  0.0464\n",
      "2  0.0000    0.0000  0.613102      0.0000  0.0000              0.0000     0.0000  0.0000      0.0000    0.0000  0.0000   0.0000    0.000000     0.0000     0.0000  0.0000     0.0000   0.0000  0.0000  0.0000  0.000000     0.0000  0.0000  0.204367     0.0000         0.0000      0.0000      0.0000     0.0000         0.0000  0.000000   0.0000  0.000000     0.0000   0.0000  0.0000  0.0000    0.000000   0.000000     0.0000     0.0000          0.0000     0.0000  0.0000  0.000000          0.0000  0.000000       0.0000             0.0000        0.0000   0.0000      0.0000      0.0000  0.0000       0.0000           0.0000       0.0000   0.0000      0.0000      0.0000       0.0000   0.0000  0.000000  0.0000  0.0000  0.0000  0.0000  0.613102  0.0000  0.0000  0.000000         0.0000    0.0000  0.0000                           0.0000       0.0000      0.0000      0.0000      0.000000   0.0000  0.0000  0.0000  0.0000  0.0000  0.204367  0.000000  0.000000  0.0000  0.0000     0.0000   0.000000      0.0000  0.0000  0.000000          0.0000       0.0000  0.0000  0.405806  0.0000     0.0000        0.0000    0.0000       0.0000    0.0000  0.0000      0.0000     0.0000   0.0000  0.000000   0.0000   0.0000  0.000000  0.000000       0.0000  0.0000           0.0000       0.0000  0.000000      0.0000     0.000000  0.000000  0.0000  0.000000  0.0000  0.0000      0.0000   0.0000     0.0000   0.0000  0.000000    0.0000      0.0000  0.0000  0.0000   0.0000  0.0000  0.000000  0.0000         0.0000  0.0000        0.0000    0.000000  0.0000  0.0000  0.000000  0.0000     0.0000  0.0000  0.0000  0.0000      0.0000   0.0000  0.0000  0.0000  0.000000   0.0000  0.0000  0.0000\n",
      "3  0.0464    0.0464  0.000000      0.0464  0.0464              0.0464     0.0464  0.0464      0.0464    0.0464  0.0464   0.0464    0.092799     0.0464     0.0464  0.0464     0.0464   0.0464  0.0464  0.0464  0.139199     0.0464  0.0464  0.000000     0.0464         0.0464      0.0464      0.0464     0.0464         0.0464  0.139199   0.0464  0.092799     0.0464   0.0464  0.0464  0.0464    0.185598   0.278397     0.0464     0.0464          0.0464     0.0464  0.0464  0.092799          0.0464  0.139199       0.0464             0.0464        0.0464   0.0464      0.0464      0.0464  0.0464       0.0464           0.0464       0.0464   0.0464      0.0464      0.0464       0.0464   0.0464  0.371196  0.0464  0.0464  0.0464  0.0464  0.000000  0.0464  0.0464  0.139199         0.0464    0.0464  0.0464                           0.0464       0.0464      0.0464      0.0464      0.139199   0.0464  0.0464  0.0464  0.0464  0.0464  0.000000  0.092799  0.092799  0.0464  0.0464     0.0464   0.092799      0.0464  0.0464  0.092799          0.0464       0.0464  0.0464  0.276402  0.0464     0.0464        0.0464    0.0464       0.0464    0.0464  0.0464      0.0464     0.0464   0.0464  0.092799   0.0464   0.0464  0.092799  0.092799       0.0464  0.0464           0.0464       0.0464  0.185598      0.0464     0.092799  0.185598  0.0464  0.231998  0.0464  0.0464      0.0464   0.0464     0.0464   0.0464  0.092799    0.0464      0.0464  0.0464  0.0464   0.0464  0.0464  0.092799  0.0464         0.0464  0.0464        0.0464    0.231998  0.0464  0.0464  0.092799  0.0464     0.0464  0.0464  0.0464  0.0464      0.0464   0.0464  0.0464  0.0464  0.092799   0.0464  0.0464  0.0464\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# âœ… Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… News sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# âœ… Collect article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return [word for word in word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "# âœ… Scrape articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:30]:  # Limit for testing\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Load DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Preprocess content\n",
    "df[\"Cleaned_Content\"] = df[\"Content\"].apply(preprocess_text)\n",
    "df[\"Tokenized\"] = df[\"Content\"].apply(tokenize)\n",
    "\n",
    "# âœ… TF-IDF Feature Engineering\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Cleaned_Content\"])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nðŸ”  TF-IDF Feature Shape:\", tfidf_df.shape)\n",
    "\n",
    "# âœ… Word2Vec Feature Engineering\n",
    "w2v_model = Word2Vec(sentences=df[\"Tokenized\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_doc_vector(tokens):\n",
    "    valid_tokens = [w for w in tokens if w in w2v_model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "    return np.mean(w2v_model.wv[valid_tokens], axis=0)\n",
    "\n",
    "df[\"W2V_Vector\"] = df[\"Tokenized\"].apply(get_doc_vector)\n",
    "\n",
    "# âœ… Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# âœ… Show results\n",
    "print(f\"\\nðŸ“ Dataset Size: {df.shape}\")\n",
    "print(\"\\nðŸ“Š Sample of Preprocessed DataFrame with Features:\")\n",
    "print(df[[\"Title\", \"Author\", \"Cleaned_Content\", \"W2V_Vector\"]].head())\n",
    "\n",
    "print(\"\\nðŸ“Š Sample TF-IDF DataFrame:\")\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "534a1f89-a1df-4e16-9fd9-7074d1318afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "âš ï¸ Error fetching links from https://dcs.uoc.ac.in/cida/resources/hwb: HTTPSConnectionPool(host='dcs.uoc.ac.in', port=443): Max retries exceeded with url: /cida/resources/hwb (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002995D6D6000>: Failed to resolve 'dcs.uoc.ac.in' ([Errno 11001] getaddrinfo failed)\"))\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "âš ï¸ Error fetching links from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198: HTTPSConnectionPool(host='figshare.com', port=443): Max retries exceeded with url: /articles/dataset/Fake_and_True_News_Dataset/13325198 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002995D6CCD70>: Failed to resolve 'figshare.com' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Cleaned_Content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Cleaned_Content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m LABELS[:\u001b[38;5;28mlen\u001b[39m(df)]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# âœ… Preprocess content\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned_Content\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned_Content\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# âœ… TF-IDF vectorization\u001b[39;00m\n\u001b[0;32m    100\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Cleaned_Content'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# âœ… Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… Dummy Labels for demonstration (since scraping doesnâ€™t give labels)\n",
    "# Use 0 = fake, 1 = real\n",
    "LABELS = [0, 1, 0, 1, 0, 1] * 10  # Extend or modify as needed\n",
    "\n",
    "# âœ… News sources (for scraping, limited in scope for now)\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\"\n",
    "]\n",
    "\n",
    "# âœ… Helper: Get article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Helper: Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# âœ… Scrape articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:10]:  # Limited to 10 articles/site for demo\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Load into DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Assign dummy labels for testing\n",
    "df = df[:len(LABELS)]  # Match size\n",
    "df[\"Label\"] = LABELS[:len(df)]\n",
    "\n",
    "# âœ… Preprocess content\n",
    "df[\"Cleaned_Content\"] = df[\"Cleaned_Content\"].apply(preprocess_text)\n",
    "\n",
    "# âœ… TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "X = vectorizer.fit_transform(df[\"Cleaned_Content\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# âœ… Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Model training and evaluation function\n",
    "def train_and_evaluate(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"\\nðŸ” {name} Results\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# âœ… Logistic Regression\n",
    "train_and_evaluate(LogisticRegression(max_iter=1000), \"Logistic Regression\")\n",
    "\n",
    "# âœ… Naive Bayes\n",
    "train_and_evaluate(MultinomialNB(), \"Naive Bayes\")\n",
    "\n",
    "# âœ… Random Forest\n",
    "train_and_evaluate(RandomForestClassifier(n_estimators=100), \"Random Forest\")\n",
    "\n",
    "# âœ… Support Vector Machine\n",
    "train_and_evaluate(SVC(kernel=\"linear\"), \"Support Vector Machine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d23f4725-e1c0-4081-9694-6ddc9a4fad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "\n",
      "ðŸ” Naive Bayes Results\n",
      "Accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Random Forest Results\n",
      "Accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "\n",
      "ðŸ§¾ Column Names in the Dataset:\n",
      "['Title', 'Author', 'Date', 'Content', 'Source', 'Label', 'Cleaned_Content']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# âœ… Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… Dummy Labels for demonstration\n",
    "LABELS = [0, 1, 0, 1, 0, 1] * 10  # Adjust size as needed\n",
    "\n",
    "# âœ… News sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\"\n",
    "]\n",
    "\n",
    "# âœ… Get article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# âœ… Scrape articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:10]:  # Demo limit\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Load into DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Assign dummy labels\n",
    "df = df[:len(LABELS)]\n",
    "df[\"Label\"] = LABELS[:len(df)]\n",
    "#['Title', 'Author', 'Date', 'Content', 'Source']\n",
    "# âœ… Preprocess text\n",
    "df[\"Cleaned_Content\"] = df[\"Source\"].apply(preprocess_text)\n",
    "\n",
    "# âœ… TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "X = vectorizer.fit_transform(df[\"Cleaned_Content\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# âœ… Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Model training and evaluation\n",
    "def train_and_evaluate(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"\\nðŸ” {name} Results\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# âœ… Train models\n",
    "# train_and_evaluate(LogisticRegression(max_iter=1000), \"Logistic Regression\")\n",
    "train_and_evaluate(MultinomialNB(), \"Naive Bayes\")\n",
    "train_and_evaluate(RandomForestClassifier(n_estimators=100), \"Random Forest\")\n",
    "# train_and_evaluate(SVC(kernel=\"linear\"), \"Support Vector Machine\")\n",
    "\n",
    "# âœ… Print column names\n",
    "print(\"\\nðŸ§¾ Column Names in the Dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "444fa47f-a74b-40f1-a0f9-f1335294d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "\n",
      "ðŸ“ Dataset Size: (4, 5)\n",
      "\n",
      "ðŸ“Š Full Scraped Dataset:\n",
      "                                                                   Title   Author        Date                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Content                                                                                                                                                                                                      Source\n",
      "0  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown  2020-08-01                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Click on â€˜Get News Alertsâ€™ to get the latest news alerts from\\n\\nNot now Get News Alerts  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "1                      Mechanism to detect fake news on health developed  Authors  2020-07-31  Researchers at the University of Calicut in collaboration with Queenâ€™s University, Belfast, United Kingdom, have developed an emotion cognizant health fake news detection mechanism to curtail disinformation and misinformation in the wake of the COVID-19 scare.\\n\\nFor the last two years, experts at the two universities have been working towards tackling fake news on health. Now, The requirement is more after the World Health Organisation (WHO) cautioned against giving false hopes to people exploiting the widespread fear of the pandemic.\\n\\nDr. Lajish V.L., head of the department of computer science, University of Calicut, said in a statement on Friday that the research method relied on an exceedingly simple mechanism, that of identifying emotionally oriented words in a news article and adding the emotion label beside them, before using detection mechanisms.\\n\\nArtificial Intelligence\\n\\nâ€œWe, at the Computational Intelligence and Data Analytics [CIDA] Lab, started our work in this area by understanding the holistic approach of this research. We used different types of advanced Machine Learning and Artificial Intelligence algorithms, especially to cultivate emotion amplification-based text representations and to conduct fake news detection experiments,â€ he said.\\n\\nExplaining the need for a high-potential fake news detection system, K. Anoop, a researcher, said the commercial and political phenomenon of empathically optimised automated fake news was on near-horizon. His research on effect-oriented fake news detection using Machine Learning was one among the AWSAR-2019 award winners for the best popular science stories, instituted by the Department of Science and Technology.\\n\\nâ€œThe simplicity of the emotion-enrichment method we propose makes it applicable for usage within a diversity of fake news detection tools,â€ said Dr. Deepak Padmanabhan, faculty member of the computer science department at Queenâ€™s University. In view of COVID-19 regulations that make travel impossible, the researchers will present their work virtually to the research community. The department of computer science, University of Calicut, has set up a web portal â€” https://dcs.uoc.ac.in/cida/break-the-fake â€” for collecting fake news in both English and Malayalam from the public for experimentation.\\n\\nThe portal will eventually evolve into a fact-finding system which will be freely available in the public domain.                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "2  à´µàµà´¯à´¾à´œà´µà´¾à´°àµâ€à´¤àµà´¤à´•à´³àµ† à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¨àµâ€ à´¨à´¿à´°àµâ€à´®à´¿à´¤à´¬àµà´¦àµà´§à´¿à´¯àµà´®à´¾à´¯à´¿ à´®à´²à´¯à´¾à´³à´¿ à´—à´µàµ‡à´·à´•à´¸à´‚à´˜à´‚  Unknown  2020-08-01                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Click on â€˜Get News Alertsâ€™ to get the latest news alerts from\\n\\nNot now Get News Alerts  https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "3                      Mechanism to detect fake news on health developed  Authors  2020-07-31  Researchers at the University of Calicut in collaboration with Queenâ€™s University, Belfast, United Kingdom, have developed an emotion cognizant health fake news detection mechanism to curtail disinformation and misinformation in the wake of the COVID-19 scare.\\n\\nFor the last two years, experts at the two universities have been working towards tackling fake news on health. Now, The requirement is more after the World Health Organisation (WHO) cautioned against giving false hopes to people exploiting the widespread fear of the pandemic.\\n\\nDr. Lajish V.L., head of the department of computer science, University of Calicut, said in a statement on Friday that the research method relied on an exceedingly simple mechanism, that of identifying emotionally oriented words in a news article and adding the emotion label beside them, before using detection mechanisms.\\n\\nArtificial Intelligence\\n\\nâ€œWe, at the Computational Intelligence and Data Analytics [CIDA] Lab, started our work in this area by understanding the holistic approach of this research. We used different types of advanced Machine Learning and Artificial Intelligence algorithms, especially to cultivate emotion amplification-based text representations and to conduct fake news detection experiments,â€ he said.\\n\\nExplaining the need for a high-potential fake news detection system, K. Anoop, a researcher, said the commercial and political phenomenon of empathically optimised automated fake news was on near-horizon. His research on effect-oriented fake news detection using Machine Learning was one among the AWSAR-2019 award winners for the best popular science stories, instituted by the Department of Science and Technology.\\n\\nâ€œThe simplicity of the emotion-enrichment method we propose makes it applicable for usage within a diversity of fake news detection tools,â€ said Dr. Deepak Padmanabhan, faculty member of the computer science department at Queenâ€™s University. In view of COVID-19 regulations that make travel impossible, the researchers will present their work virtually to the research community. The department of computer science, University of Calicut, has set up a web portal â€” https://dcs.uoc.ac.in/cida/break-the-fake â€” for collecting fake news in both English and Malayalam from the public for experimentation.\\n\\nThe portal will eventually evolve into a fact-finding system which will be freely available in the public domain.                                                                                        https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "Column names of the dataset:\n",
      "['Title', 'Author', 'Date', 'Content', 'Source']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "# âœ… Use a mix of real and fake news sources (2â€“3 total for demo/testing)\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# Function to collect article URLs\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))  # Remove duplicates\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract article details using Newspaper3k\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Scrape and collect articles\n",
    "articles_data = []\n",
    "\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    \n",
    "    for link in links[:300]:  # Limit to 30 articles per site\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Convert to DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Set pandas options to print all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# âœ… Print full dataset\n",
    "print(f\"\\nðŸ“ Dataset Size: {df.shape}\")\n",
    "print(\"\\nðŸ“Š Full Scraped Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Print the column names of the DataFrame\n",
    "print(\"Column names of the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbe531fb-c1a7-4f98-a011-2847cb00e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "\n",
      "ðŸ” Naive Bayes Results\n",
      "Accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "\n",
      "ðŸ§¾ Column Names in the Dataset:\n",
      "['Title', 'Author', 'Date', 'Content', 'Source', 'Label', 'Cleaned_Content']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# âœ… Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… Dummy Labels for demonstration\n",
    "LABELS = [0, 1, 0, 1, 0, 1] * 10  # Adjust size as needed\n",
    "\n",
    "# âœ… News sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\"\n",
    "]\n",
    "\n",
    "# âœ… Get article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# âœ… Scrape articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:10]:  # Demo limit\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Load into DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Assign dummy labels\n",
    "df = df[:len(LABELS)]\n",
    "df[\"Label\"] = LABELS[:len(df)]\n",
    "\n",
    "# âœ… Preprocess text\n",
    "df[\"Cleaned_Content\"] = df[\"Source\"].apply(preprocess_text)\n",
    "\n",
    "# âœ… TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "X = vectorizer.fit_transform(df[\"Cleaned_Content\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# âœ… Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Model training and evaluation\n",
    "def train_and_evaluate(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"\\nðŸ” {name} Results\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# âœ… Train only Naive Bayes model\n",
    "train_and_evaluate(MultinomialNB(), \"Naive Bayes\")\n",
    "\n",
    "# âœ… Print column names\n",
    "print(\"\\nðŸ§¾ Column Names in the Dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ab90f93-1039-4a53-b4b4-8f19e485b27c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2830227997.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[40], line 52\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"Content\": article.text\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# âœ… Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… Dummy Labels for demonstration\n",
    "LABELS = [0, 1, 0, 1, 0, 1] * 10  # Adjust size as needed\n",
    "\n",
    "# âœ… News sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\"\n",
    "]\n",
    "\n",
    "# âœ… Get article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8b88465-a040-4447-99a4-0e78ca3e280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "\n",
      "ðŸ” Naive Bayes Results\n",
      "Accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "\n",
      "ðŸ§¾ Column Names in the Dataset:\n",
      "['Title', 'Author', 'Date', 'Content', 'Source', 'Label', 'Cleaned_Content']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# âœ… Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# âœ… Dummy Labels for demonstration\n",
    "LABELS = [0, 1, 0, 1, 0, 1] * 10  # Adjust size as needed\n",
    "\n",
    "# âœ… News sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\"\n",
    "]\n",
    "\n",
    "# âœ… Get article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# âœ… Extract article content\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# âœ… Scrape articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:10]:  # Demo limit\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "# âœ… Load into DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# âœ… Assign dummy labels\n",
    "df = df[:len(LABELS)]\n",
    "df[\"Label\"] = LABELS[:len(df)]\n",
    "\n",
    "# âœ… Preprocess text\n",
    "df[\"Cleaned_Content\"] = df[\"Content\"].apply(preprocess_text)\n",
    "\n",
    "# âœ… TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "X = vectorizer.fit_transform(df[\"Cleaned_Content\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# âœ… Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Model training and evaluation\n",
    "def train_and_evaluate(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"\\nðŸ” {name} Results\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# âœ… Train Naive Bayes model\n",
    "naive_bayes_model = MultinomialNB(alpha=1.0)  # You can adjust alpha for smoothing\n",
    "train_and_evaluate(naive_bayes_model, \"Naive Bayes\")\n",
    "\n",
    "# âœ… Print column names\n",
    "print(\"\\nðŸ§¾ Column Names in the Dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f544d9a3-8fea-452e-acca-d6099d18020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "ðŸ” Scraping from: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22Online%20news%20media%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22Online%20news%20media%22\n",
      "ðŸ“Œ Processing: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "âš ï¸ Error extracting article from https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22: Article `download()` failed with 404 Client Error: Not Found for url: https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword:%20%22news%20articles%22 on URL https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198/search?q=:keyword: %22news%20articles%22\n",
      "ðŸ” Scraping from: https://dcs.uoc.ac.in/cida/resources/hwb\n",
      "ðŸ“Œ Processing: https://www.mathrubhumi.com/technology/news/artificial-intelligence-to-identify-fake-news-developed-by-malayali-researchers-1.4946765?fbclid=IwAR0brcri9jUN2THvFJRudXb6Q0tx7FIIryigNKmlQsgUQ4JBKPCf5UV6eac\n",
      "ðŸ“Œ Processing: news/mathrubhumi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/mathrubhumi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/mathrubhumi-1aug2020.png' on URL :/news/mathrubhumi-1aug2020.png\n",
      "ðŸ“Œ Processing: news/newindianexpress-2aug2020.jpeg\n",
      "âš ï¸ Error extracting article from news/newindianexpress-2aug2020.jpeg: Article `download()` failed with No connection adapters were found for ':/news/newindianexpress-2aug2020.jpeg' on URL :/news/newindianexpress-2aug2020.jpeg\n",
      "ðŸ“Œ Processing: news/deshabhimani-02aug2020.jpg\n",
      "âš ï¸ Error extracting article from news/deshabhimani-02aug2020.jpg: Article `download()` failed with No connection adapters were found for ':/news/deshabhimani-02aug2020.jpg' on URL :/news/deshabhimani-02aug2020.jpg\n",
      "ðŸ“Œ Processing: https://www.thehindu.com/news/cities/kozhikode/mechanism-to-detect-fake-news-on-health-developed/article32244004.ece\n",
      "ðŸ“Œ Processing: news/hindu-print-kochi-1aug2020.png\n",
      "âš ï¸ Error extracting article from news/hindu-print-kochi-1aug2020.png: Article `download()` failed with No connection adapters were found for ':/news/hindu-print-kochi-1aug2020.png' on URL :/news/hindu-print-kochi-1aug2020.png\n",
      "\n",
      "ðŸ”  TF-IDF Feature Shape: (4, 158)\n",
      "\n",
      "âœ… Accuracy score on test dataset: 1.0\n",
      "\n",
      "ðŸ’¬ Training Word2Vec model...\n",
      "âœ… Word2Vec training complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to collect article links\n",
    "def get_article_links(news_url, keyword=\"news\"):\n",
    "    try:\n",
    "        response = requests.get(news_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if keyword in a[\"href\"]]\n",
    "        full_links = [news_url + link if link.startswith(\"/\") else link for link in links]\n",
    "        return list(set(full_links))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error fetching links from {news_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract article details\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            \"Title\": article.title.strip() if article.title else \"No Title\",\n",
    "            \"Author\": \", \".join(article.authors) if article.authors else \"Unknown\",\n",
    "            \"Date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
    "            \"Content\": article.text.strip() if article.text else \"No Content\",\n",
    "            \"Source\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Tokenization function for Word2Vec\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return [word for word in word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "# Define news sources\n",
    "NEWS_SOURCES = [\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\",\n",
    "    \"https://figshare.com/articles/dataset/Fake_and_True_News_Dataset/13325198\",\n",
    "    \"https://dcs.uoc.ac.in/cida/resources/hwb\"\n",
    "]\n",
    "\n",
    "# Collect articles\n",
    "articles_data = []\n",
    "for source in NEWS_SOURCES:\n",
    "    print(f\"ðŸ” Scraping from: {source}\")\n",
    "    links = get_article_links(source)\n",
    "    for link in links[:30]:  # Limit for demo/testing\n",
    "        print(f\"ðŸ“Œ Processing: {link}\")\n",
    "        data = extract_article_details(link)\n",
    "        if data:\n",
    "            articles_data.append(data)\n",
    "        time.sleep(1)  # To avoid hitting servers too fast\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# Handle empty content gracefully\n",
    "df = df[df[\"Content\"].str.strip().astype(bool)]\n",
    "\n",
    "# Preprocess and tokenize text\n",
    "df[\"Cleaned_Content\"] = df[\"Content\"].apply(preprocess_text)\n",
    "df[\"Tokenized\"] = df[\"Content\"].apply(tokenize)\n",
    "\n",
    "# TF-IDF Feature Engineering\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Cleaned_Content\"])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nðŸ”  TF-IDF Feature Shape:\", tfidf_df.shape)\n",
    "\n",
    "# Dummy target labels for classification (replace with actual if available)\n",
    "df['Label'] = [1 if i % 2 == 0 else 0 for i in range(len(df))]  # 1 = Real, 0 = Fake\n",
    "\n",
    "# Prepare train/test data\n",
    "X = tfidf_df\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Naive Bayes Model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nâœ… Accuracy score on test dataset:\", accuracy)\n",
    "\n",
    "# Word2Vec (optional but useful)\n",
    "print(\"\\nðŸ’¬ Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(sentences=df[\"Tokenized\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"âœ… Word2Vec training complete!\")\n",
    "\n",
    "# You can now use: w2v_model.wv.most_similar(\"fake\"), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349987c5-02af-44b5-9e4c-a317f3ec5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5cfe0cf-c6a7-42cb-9981-018fa2884ced",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, AdamW\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âœ… Step 1: BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# âœ… Step 2: Custom Dataset\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# âœ… Step 3: Prepare Data for BERT\n",
    "X_texts = df['Cleaned_Content'].tolist()\n",
    "y_labels = df['Label'].tolist()\n",
    "\n",
    "X_train_texts, X_test_texts, y_train_labels, y_test_labels = train_test_split(\n",
    "    X_texts, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = FakeNewsDataset(X_train_texts, y_train_labels, tokenizer)\n",
    "test_dataset = FakeNewsDataset(X_test_texts, y_test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# âœ… Step 4: BERT Model for Classification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# âœ… Step 5: Training Loop\n",
    "EPOCHS = 2\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"ðŸš€ Training Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"âœ… Epoch {epoch+1} complete!\")\n",
    "\n",
    "# âœ… Step 6: Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nðŸ§¾ Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
